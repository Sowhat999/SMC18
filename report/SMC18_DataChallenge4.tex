\documentclass{article}

\usepackage{listings}
\pagenumbering{gobble}
\usepackage[english]{babel}
\usepackage[pdftex]{color,graphicx}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*,noitemsep}
\setlist[enumerate]{leftmargin=*,noitemsep}
\usepackage[T1]{fontenc}
\setlength\parindent{0pt}

\title{SMC 2018 Data Challenge \#4}
\author{Ketan Maheshwari (km0@ornl.gov)}

\begin{document}
%\begin{titlepage}
%   \vspace*{\stretch{1.0}}
%   \begin{center}
%      \Large\textbf{SMC 2018 Data Challenge \#4}\\
%      \large\textit{Ketan Maheshwari, km0@ornl.gov}
%   \end{center}
%   \vspace*{\stretch{2.0}}
%\end{titlepage}
\maketitle
\section*{Introduction}

I report on my response to the SMC 2018 Data Challenge 4--Acedamic Data Mining.
The challenge is addressed by employing the classic Linux tools and a modern HPC
parallel scripting platform.

This report describes the tools used, their rationale, solutions to the
problems and a glance at the results. Detailed solutions and results may be
found here: https://github.com/ketancmaheshwari/SMC18

\section*{Tools used}
\subsection*{Software}
Classic Linux tools such as \texttt{awk}, \texttt{sort}, \texttt{grep},
\texttt{tr}, \texttt{sed} and \texttt{bash} are used.  The \texttt{jq} tool was
used for initial data conversion from json to awk-suitable table format. For
the bulk of processing, I use awk. Syntax in awk programs is known to be terse
and hard to read by some accounts. I have taken special care to make the awk
programs as readable as possible.  ANL's Swift (\texttt{swift-lang.org/Swift-T}) parallel scripting tool was used
to run the awk programs in parallel over the dataset to radically improve
performance. Swift uses MPI based communication and load-balancing model to
parallelize over shared as well as distributed memory architectures.

\subsection*{Hardware}
A large-memory (24 T) SGI system with 512-core Intel Xeon (2.5GHz) was used for
all the processing.

\subsection*{Rationale}
\begin{enumerate}
\item \texttt{Awk} is concise, efficient and expressive for text processing.
\item Alternative tools such as modern Python libraries have speed limitations, portability concerns and considerable learning curve.
\item SQL based tools and databases are hard to parallelize.
\item Traditional Linux tools are portable and available on almost all the standard Linux distributions.
\item Swift was used over GNU parallel for its portability to both shared memory systems and distributed memory clusters.
\end{enumerate}

\section*{Data and Problems}
The data was originally in two sets of 167 json files (total 334 files) each
containing a million records. The data also consisted of a file that recorded
the list of duplicate records. An \texttt{awk} script (\texttt{src/filterdup.awk}) was used to exclude the
duplicate records from the aminer dataset. As a result, there are over 256
million (256,382,605 to be exact) unique records to process. The data occupies
329G of disk space. Several fields in the data are \texttt{null}. Those records
were avoided where relevant. Additionally, records related to non-English
publications were avoided where needed.

The json files were converted to tabular files with 19 original columns and one
additional column called ``num\_authors" showing the number of authors for a given
publication record.

\subsection*{Auxiliary data}
In addition to the existing data, I use four lists: 1) List of cities of
world with population 100,000 or above; 2) List of all countries;
3) List of all the universities and research institutes; 4) A list of common
words to avoid in some of the results (eg. the, from, after, etc.)

\section*{Solutions}
\subsection*{Preprocessing}
The original data is in json format. The jq tool was used to transform it to
tabular format (\texttt{src/json2tabular.sh}). Further curation of table data
was done by removing extraneous space, square brackets, escape characters and
quotes using \texttt{sed}.

\subsection*{Parallelization and Speedup}
Each of the solution is run in parallel over the 334 data files. This has
resulted in radical speedup of each of the problems. As mentioned in the
description of the solutions in the sections below, none of the solution has
taken more than a minute of runtime.

\subsection*{Problem1}
The parallel implementation of the task finishes in 25 seconds.
\subsection*{Problem2}
The parallel implementation of the task finishes in 9 minutes.
\subsection*{Problem3}
The parallel implementation of the task finishes in 25 seconds.
\subsection*{Problem4}
This problem may be solved in three distinct ways. Parallelizing part-3 was
most challenging because it involved a two-level parallel nested foreach loop.
The outer loop iterates over the years and the inner loop iterates over the
input files.

The parallel implementation of the task finishes in 48 minutes.
\subsection*{Problem5}
The parallel implementation of the task finishes in 26 seconds.

\subsection*{Postprocessing}
Some of the results obtained were postprocessed for visulization using D3
graphics framework and ffmpeg libraries.

\section*{Results}

\section*{Summary}
In this work I show how the time tested Linux tools can be leveraged to solve
modern problems. I show that hundreds or millions of records may be processed
in under a minute using Swift. I show the results that offer insight into the
data without employing any formal sophisticated algorithms.

\section*{Acknowledgements.}
I acknowledge Brian Zachary, Suhas Somnath, Drew Schmidt for their helpful
inputs. I acknowledge CADES.
\end{document}
